\chapter{Теоретическая часть}

% \section{Введение}


\section{Постановка задачи}
В данной лабораторной работе решается задача кластеризации текстовых документов. Для заданного набора текстов необходимо:
\begin{enumerate}
    \item Преобразовать текстовые документы в векторное представление;
    \item Применить методы кластеризации: k-средних, c-средних и Гат-Гевы;
    \item Проанализировать качество кластеризации при различном количестве кластеров, определить среднее внутрикластерное расстояние и среднее межкластерное расстояние для каждого рассматриваемого случая.
\end{enumerate}

\section{Предобработка текстовых данных}

\subsection{Выделение словоформ}
Словоформа — это конкретная грамматическая форма слова, встречающаяся в тексте. Процесс выделения словоформ из текста включает:
\begin{itemize}
    \item Разбиение текста на отдельные слова;
    \item Приведение к нижнему регистру;
    \item Удаление служебных частей речи;
    \item Удаление пунктуации и специальных символов.
\end{itemize}
В данном подходе слова сохраняются в том виде, в котором они встречаются в тексте.

\subsection{Выделение начальных форм слов}
Лемматизация — процесс приведения слова к его начальной форме (лемме). В рамках этой работы применялась библиотека \texttt{pymorphy3}, которая:
\begin{itemize}
    \item Определяет часть речи слова
    \item Учитывает морфологические характеристики (род, число, падеж и др.)
    \item Возвращает нормальную форму слова
\end{itemize}
Преимущество лемматизации можно назвать уменьшение размерности пространства признаков за счет объединения различных форм одного слова. В нашем случае, лемматизация приводит к уменьшению размерности вектора текстового документа.

\section{Векторизация документов}
После предобработки тексты преобразуются в числовые векторы. Преобразовать текст можно различными способами. В рамках работы рассмотрим статические методы векторизации.

В основе статистических подходов лежит парадигма «Мешка слов», которая абстрагируется от порядка слов и рассматривает текст как неупорядоченную коллекцию терминов. Эти методы основаны на подсчёте частот слов, что приводит к созданию векторных представлений высокой размерности. Несмотря на свою простоту и интерпретируемость, они обладают общими недостатками, такими как семантическая слепота, пренебрежение порядком слов и проблема разреженности данных.

\subsection{One-Hot Encoding}

Самый просто и примитивный метод векторизации текста, результатом которого является матрица с единицами и нулями внутри. 1 говорит о том, что какой-то текстовый элемент встречается в предложении (или в нашем случае документе). 0 говорит о том, что элемент не встречается в предложении.

\subsection{Term Frequency-Inverse Document Frequency}

Term Frequency-Inverse Document Frequency или сокращенно TF-IDF — один из наиболее распространённых и эффективных статистических методов. Вес слова вычисляется как произведение двух компонент: количество раз, когда слово встретилось в документе и натурального логарифма от количества документов деленное на количество документов содержащее этот символ. Формула для расчета представлена в (\ref{equ:tfidf}). 

\begin{equation}\label{equ:tfidf}
    w_{i,j} = tf_{i,j} \times \log\frac{N}{df_i}
\end{equation}
где:
\begin{itemize}
    \item $tf_{i,j}$ — количество раз, когда слово $i$ встретилось в документе $j$;
    \item $df_i$ — количество документов содержащее символ $i$;
    \item $N$ — общее количество документов.
\end{itemize}

Логарифмическая мера снижает влияние служебных частей речи и повышает значимость слов, характерных для конкретного документа.

\subsection{N-граммы}

Для учёта локального контекста и фиксации устойчивых словосочетаний применяется расширение классического подхода — N-граммы. Элементами векторизации становятся не отдельные слова (униграммы), а последовательности из N соседних слов или символов. Это позволяет моделировать такие выражения, как «машинное обучение», в виде единого семантического токена. Однако использование N-грамм приводит к комбинаторному росту размерности пространства признаков, что создаёт серьёзные вычислительные сложности и требует больше данных для обучения моделей.

\section{Методы кластеризации}


\subsection{Метод k-средних}
K-средних — классический метод четкой кластеризации. Четкая кластеризация -- кластеризация с заранее известным количеством кластеров. Алгоритм можно описать следующим образом:
\begin{enumerate}
    \item Начальный выбор координат центроидов $k$ кластеров/ Выбор,как правило, носит случайный характер, однако существуют модификации метода, где начальный выбор выполняется на основе поиска максимально отдалённых друг от друга потенциальных центроидов кластеров;
    \item Назначение каждого объекта ближайшему центроиду;
    \item Пересчет центроидов кластеров;
    \item Повторение шагов 2-3 до сходимости.
\end{enumerate}
Целевая функция:
\begin{equation}
    J = \sum_{i=1}^k \sum_{x \in C_i} \|x - \mu_i\|^2
\end{equation}

\subsection{Метод c-средних}

Метод с-средних -- метод нечёткой кластеризации, где каждый объект принадлежит всем кластерам с определённой степенью принадлежности от 0 до 1.

Метод минимизирует взвешенную сумму квадратов расстояний:

\begin{equation}
J_m(U, V) = \sum_{i=1}^{c} \sum_{j=1}^{n} (u_{ij})^m \, d_{ij}^2
\end{equation}

где:
\begin{itemize}
    \item \( d_{ij} = \| \mathbf{x}_j - \mathbf{v}_i \| \) — евклидово расстояние
    \item \( V = \{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_c \} \) — множество центроидов
    \item \( m > 1 \) -- параметр нечёткости.
\end{itemize}

Алгоритм метода с-средних можно описать следующим образом.


\begin{enumerate}
\item Пересчёт центров кластеров. Центр каждого кластера вычисляется как взвешенное среднее всех точек данных. Весом для точки служит её степень принадлежности к этому кластеру, возведённая в степень $m$. Таким образом, точки с высокой степенью принадлежности сильнее влияют на положение центра;
\item Для каждой точки данных заново вычисляется её принадлежность ко всем кластерам. Степень принадлежности обратно пропорциональна расстоянию от точки до центра кластера: чем точка ближе к центру, тем выше её принадлежность к этому кластеру. Для одной точки сумма всех принадлежностей равна 1;
\item Вычисления прекращаются, когда изменения в матрице принадлежностей между двумя последовательными итерациями становятся меньше заданного порога $\varepsilon$;
\end{enumerate}

\subsection{Метод Гат-Гевы}
Метод Гат-Гевы — модификация алгоритма c-средних, который используется адаптивная метрика расстояния (\textit{англ. fuzzy maximum likelihood estimation, FMLE}) вместо стандартной евклидовой.

Алгоритм можно описать следующим образом:
\begin{enumerate}
    \item Случайным образом задаются начальные степени принадлежности точек к кластерам;
    \item Центроиды вычисляются как взвешенные средние всех точек, где веса — степени принадлежности в степени $m$;
    \item Для каждого кластера вычисляется ковариационная матрица, которая описывает:
        \begin{itemize}
            \item Направление наибольшего разброса точек;
            \item Степень вытянутости кластера;
            \item Ориентацию кластера в пространстве.
        \end{itemize}
    \item Расстояние от точки до центроиды кластера вычисляется с учётом его формы. Точка может быть ближе к центру вытянутого кластера по его длинной оси, даже если евклидово расстояние велико;
    \item Степени принадлежности пересчитываются на основе новых расстояний;
    \item Процесс повторяется, пока изменения в матрице принадлежностей не станут меньше заданного значения.
\end{enumerate}

\section{Метрики оценки качества кластеризации}

\textbf{Среднее внутрикластерное расстояние} показывает насколько похожи объекты внутри одного кластера. Вычисляется по формуле (\ref{equ:sr1}):
\begin{equation}\label{equ:sr1}
    W = \frac{1}{N} \sum_{i=1}^k \sum_{x \in C_i} \|x - \mu_i\|
\end{equation}
где $N$ — общее количество объектов, $\mu_i$ — центроид кластера $C_i$.

\textbf{Среднее межкластерное расстояние} показывает насколько хорошо кластеры отделены друг от друга. Вычисляется по формуле (\ref{equ:sr2}):
\begin{equation}\label{equ:sr2}
    B = \frac{2}{k(k-1)} \sum_{i=1}^{k-1} \sum_{j=i+1}^k \|\mu_i - \mu_j\|
\end{equation}
где $л$ — число кластеров.

% \includeimage
%     {tree} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
%     {f} % Обтекание (без обтекания)
%     {h} % Положение рисунка (см. figure из пакета float)
%     {0.8\textwidth} % Ширина рисунка
%     {Схема предметной области} % Подпись рисунка
